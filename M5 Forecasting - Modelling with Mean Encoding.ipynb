{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting - Modelling with Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: int64(1941), object(6)\n",
      "memory usage: 452.9+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   store_id    object \n",
      " 1   item_id     object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sell_price  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 208.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Load library \n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from category_encoders import TargetEncoder \n",
    "\n",
    "# Load data\n",
    "sales = pd.read_csv('data/sales_train_evaluation.csv')\n",
    "prices = pd.read_csv('data/sell_prices.csv')\n",
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "\n",
    "# Adding sales for test data: d_1942-d_1969\n",
    "# for d in range(1942, 1970):\n",
    "#     col = 'd_' + str(d)\n",
    "#     sales[col] = 0\n",
    "#     sales[col] = sales[col].astype(np.int16)\n",
    "    \n",
    "print(sales.info())\n",
    "print(prices.info())\n",
    "print(calendar.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downcasting\n",
    "\n",
    "Credit: __[Time Series Forecasting - EDA, FE & Modelling](https://www.kaggle.com/anshuls235/time-series-forecasting-eda-fe-modelling)__\n",
    "\n",
    "We see above the sales dataset 1941 int64 columns and 6 objects corresponding to the categorical variables in the data. The prices dataset has 4 columns: 2 objects which are the categorical variables, an int64 column and a float64 column both of which are numeric. \n",
    "\n",
    "Using the min and max value of a column, each column can be converted to a subtype that uses less memory, thus reducing the size of that column and saving memory. Subtypes are as follows\n",
    "- int8/uint8: 1 byte\n",
    "- float16/int16/uint16: 2 bytes\n",
    "- float32/int32/uint32: 4 bytes\n",
    "- float64/int64/uint64: 8 bytes\n",
    "\n",
    "I will use the function below to downcast the data. This is used again after creating a merged dataset including all the sales, prices and calendar information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to downcast data\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    \n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "                \n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "                \n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sales = downcast(sales)\n",
    "prices = downcast(prices)\n",
    "calendar = downcast(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After downcasting**\n",
    "\n",
    "The reduction in size is significant. The sales dataframe reduces from 454.5 MB to 96.6 MB. Prices dataframe reduces from 208.8 MB to 45.8 MB. The calendar dataframe was small in size to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: category(6), int16(1317), int8(624)\n",
      "memory usage: 96.6 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype   \n",
      "---  ------      -----   \n",
      " 0   store_id    category\n",
      " 1   item_id     category\n",
      " 2   wm_yr_wk    int16   \n",
      " 3   sell_price  float16 \n",
      "dtypes: category(2), float16(1), int16(1)\n",
      "memory usage: 45.8 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1969 non-null   datetime64[ns]\n",
      " 1   wm_yr_wk      1969 non-null   int16         \n",
      " 2   weekday       1969 non-null   category      \n",
      " 3   wday          1969 non-null   int8          \n",
      " 4   month         1969 non-null   int8          \n",
      " 5   year          1969 non-null   int16         \n",
      " 6   d             1969 non-null   category      \n",
      " 7   event_name_1  162 non-null    category      \n",
      " 8   event_type_1  162 non-null    category      \n",
      " 9   event_name_2  5 non-null      category      \n",
      " 10  event_type_2  5 non-null      category      \n",
      " 11  snap_CA       1969 non-null   int8          \n",
      " 12  snap_TX       1969 non-null   int8          \n",
      " 13  snap_WI       1969 non-null   int8          \n",
      "dtypes: category(6), datetime64[ns](1), int16(2), int8(5)\n",
      "memory usage: 144.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sales.info())\n",
    "print(prices.info())\n",
    "print(calendar.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to modelling, the sales, prices and calendar data sets must be combined together. The function below creates the full dataset used to create the training and test datasets during modelling and prediction. One of the variables it takes as input is `first_day` which is the day number at which the train/test dataset begins (default is day 1200).\n",
    "\n",
    "The sales data is converted to leng format using the pandas `melt` function. Using pandas `merge` function, the calendar dataset is merged with the sales data on the common indices `d`. The prices dataset is them merged using common indices `store_id`, `item_id` and `wm_yr_wk`.\n",
    "\n",
    "Finally, categorical variables in the dataset need to be encoded. The categorical variables here are `item_id`, `dept_id`, `cat_id`, `store_id` and `state_id`. For each categorical variable, I encode the variable by calculating the mean sales for each group of the categorical variable. This is a form of target encoding as sales is the target variable in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "nhz = 28\n",
    "max_lags = 57\n",
    "tr_last = 1913 + 28\n",
    "fday = datetime(2016, 4, 25) + timedelta(days=28)\n",
    "\n",
    "## Function to Create dataset\n",
    "def create_data(istrain=True, nrows=True, first_day=1200, encode='mean'):\n",
    "    for col,col_dtype in calendar.dtypes.items():\n",
    "        if str(col_dtype) == 'category' and col != 'd':\n",
    "            calendar[col] = calendar[col].cat.codes.astype('int16')\n",
    "            calendar[col] -= calendar[col].min()\n",
    "            \n",
    "    \n",
    "    start_day = max(1 if istrain else tr_last-max_lags, first_day)\n",
    "    if istrain:\n",
    "        numcols = [f'd_{d}' for d in range(start_day, tr_last+1)]\n",
    "    else:\n",
    "        numcols = [f'd_{d}' for d in range(start_day, tr_last+1+28)]\n",
    "    catcols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "            \n",
    "    if not istrain:\n",
    "        for d in range(tr_last+1, tr_last+1+28):\n",
    "            sales[f'd_{d}'] = np.nan\n",
    "            \n",
    "    df = pd.melt(sales[catcols+numcols], \n",
    "                 id_vars=catcols, \n",
    "                 value_vars=[col for col in sales[catcols+numcols].columns if col.startswith('d_')], \n",
    "                 var_name='d', \n",
    "                 value_name='sales')\n",
    "    \n",
    "    df = df.merge(calendar, on='d')\n",
    "    df = df.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "    \n",
    "    if encode == 'mean': #check for now, will include other encoders in the future\n",
    "        meancols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "        for col in meancols:\n",
    "            df[f'{col}_enc'] = df[[col, 'sales']].groupby(col)['sales'].transform(lambda x: x.mean(skipna=True))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>item_id_enc</th>\n",
       "      <th>dept_id_enc</th>\n",
       "      <th>cat_id_enc</th>\n",
       "      <th>store_id_enc</th>\n",
       "      <th>state_id_enc</th>\n",
       "      <th>event_name_1_enc</th>\n",
       "      <th>event_type_1_enc</th>\n",
       "      <th>event_name_2_enc</th>\n",
       "      <th>event_type_2_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.970703</td>\n",
       "      <td>0.272927</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.671637</td>\n",
       "      <td>1.594058</td>\n",
       "      <td>1.528121</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.388101</td>\n",
       "      <td>1.388101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>4.339844</td>\n",
       "      <td>2.098430</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.671637</td>\n",
       "      <td>1.594058</td>\n",
       "      <td>1.528121</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.388101</td>\n",
       "      <td>1.388101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>2.480469</td>\n",
       "      <td>0.750691</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.671637</td>\n",
       "      <td>1.594058</td>\n",
       "      <td>1.528121</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.388101</td>\n",
       "      <td>1.388101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.328455</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.671637</td>\n",
       "      <td>1.594058</td>\n",
       "      <td>1.528121</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.388101</td>\n",
       "      <td>1.388101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.769531</td>\n",
       "      <td>0.802002</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.671637</td>\n",
       "      <td>1.594058</td>\n",
       "      <td>1.528121</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.39253</td>\n",
       "      <td>1.388101</td>\n",
       "      <td>1.388101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_008_CA_1_evaluation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_009_CA_1_evaluation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id      d  sales       date  wm_yr_wk  ...  sell_price  item_id_enc  \\\n",
       "0       CA  d_350      0 2012-01-13     11150  ...    3.970703     0.272927   \n",
       "1       CA  d_350      2 2012-01-13     11150  ...    4.339844     2.098430   \n",
       "2       CA  d_350      0 2012-01-13     11150  ...    2.480469     0.750691   \n",
       "3       CA  d_350      0 2012-01-13     11150  ...    0.500000     4.328455   \n",
       "4       CA  d_350      2 2012-01-13     11150  ...    1.769531     0.802002   \n",
       "\n",
       "   dept_id_enc  cat_id_enc  store_id_enc  state_id_enc  event_name_1_enc  \\\n",
       "0     0.816681    0.671637      1.594058      1.528121           1.39253   \n",
       "1     0.816681    0.671637      1.594058      1.528121           1.39253   \n",
       "2     0.816681    0.671637      1.594058      1.528121           1.39253   \n",
       "3     0.816681    0.671637      1.594058      1.528121           1.39253   \n",
       "4     0.816681    0.671637      1.594058      1.528121           1.39253   \n",
       "\n",
       "   event_type_1_enc  event_name_2_enc  event_type_2_enc  \n",
       "0           1.39253          1.388101          1.388101  \n",
       "1           1.39253          1.388101          1.388101  \n",
       "2           1.39253          1.388101          1.388101  \n",
       "3           1.39253          1.388101          1.388101  \n",
       "4           1.39253          1.388101          1.388101  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = create_data(istrain=True, first_day=350, encode='mean')\n",
    "\n",
    "# for col,col_dtype in calendar.dtypes.items():\n",
    "#     print('variable '+str(col)+' is dtype '+str(col_dtype))\n",
    "\n",
    "# print(calendar.dtypes.items)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41571939 entries, 0 to 41571938\n",
      "Data columns (total 31 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                category      \n",
      " 1   item_id           category      \n",
      " 2   dept_id           category      \n",
      " 3   cat_id            category      \n",
      " 4   store_id          category      \n",
      " 5   state_id          category      \n",
      " 6   d                 object        \n",
      " 7   sales             int16         \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           int16         \n",
      " 11  wday              int8          \n",
      " 12  month             int8          \n",
      " 13  year              int16         \n",
      " 14  event_name_1      int16         \n",
      " 15  event_type_1      int16         \n",
      " 16  event_name_2      int16         \n",
      " 17  event_type_2      int16         \n",
      " 18  snap_CA           int8          \n",
      " 19  snap_TX           int8          \n",
      " 20  snap_WI           int8          \n",
      " 21  sell_price        float16       \n",
      " 22  item_id_enc       float64       \n",
      " 23  dept_id_enc       float64       \n",
      " 24  cat_id_enc        float64       \n",
      " 25  store_id_enc      float64       \n",
      " 26  state_id_enc      float64       \n",
      " 27  event_name_1_enc  float64       \n",
      " 28  event_type_1_enc  float64       \n",
      " 29  event_name_2_enc  float64       \n",
      " 30  event_type_2_enc  float64       \n",
      "dtypes: category(6), datetime64[ns](1), float16(1), float64(9), int16(8), int8(5), object(1)\n",
      "memory usage: 4.9+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset occupied 4.9GB of memory. I will attempt to downcast the data again to reduce the amount of space occupied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41571939 entries, 0 to 41571938\n",
      "Data columns (total 31 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                category      \n",
      " 1   item_id           category      \n",
      " 2   dept_id           category      \n",
      " 3   cat_id            category      \n",
      " 4   store_id          category      \n",
      " 5   state_id          category      \n",
      " 6   d                 category      \n",
      " 7   sales             int16         \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           int8          \n",
      " 11  wday              int8          \n",
      " 12  month             int8          \n",
      " 13  year              int16         \n",
      " 14  event_name_1      int8          \n",
      " 15  event_type_1      int8          \n",
      " 16  event_name_2      int8          \n",
      " 17  event_type_2      int8          \n",
      " 18  snap_CA           int8          \n",
      " 19  snap_TX           int8          \n",
      " 20  snap_WI           int8          \n",
      " 21  sell_price        float16       \n",
      " 22  item_id_enc       float16       \n",
      " 23  dept_id_enc       float16       \n",
      " 24  cat_id_enc        float16       \n",
      " 25  store_id_enc      float16       \n",
      " 26  state_id_enc      float16       \n",
      " 27  event_name_1_enc  float16       \n",
      " 28  event_type_1_enc  float16       \n",
      " 29  event_name_2_enc  float16       \n",
      " 30  event_type_2_enc  float16       \n",
      "dtypes: category(7), datetime64[ns](1), float16(10), int16(3), int8(10)\n",
      "memory usage: 2.4 GB\n",
      "None\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = downcast(df)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downcasting the dataset now occupies 2.4GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Creating some new features from existing information in the data, that can be used to better predict the outcome/target variable. In this case the outcome to be predicted is 28 day sales forecasts for all items in the dataset. Here we want to create some meaningful outcomes that will help better predict these 28-day sales. Some of these new features include:\n",
    "\n",
    "* 7 and 28-day lag sales for each unique id.\n",
    "* Rolling means for the 7 and 28-day lag sales for each unique id.\n",
    "* Additional time information such as day of the week, week of the year, month , quarter, year and day of the month. These will help capture some of the seasonality present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create new features\n",
    "\n",
    "def create_features(data):\n",
    "    # Create lags and rolling means\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f'lag_{lag}' for lag in lags]\n",
    "\n",
    "    for lag,lag_col in zip(lags, lag_cols):\n",
    "        data[lag_col] = data[['id', 'sales']].groupby('id')['sales'].shift(lag)\n",
    "\n",
    "    windows = [7, 28]\n",
    "    for win in windows:\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            data[f'rmean_{lag}_{win}'] = data[['id', lag_col]].groupby('id')[lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "\n",
    "    # Create time variables\n",
    "    date_feats = {'wday':'weekday', \n",
    "                  'week':'weekofyear', \n",
    "                  'month':'month', \n",
    "                  'quarter':'quarter',\n",
    "                  'year':'year',\n",
    "                  'days':'day'}\n",
    "\n",
    "    for date_name,date_attr in date_feats.items():\n",
    "        if date_name in data.columns:\n",
    "            data[date_name] = data[date_name].astype('int16')\n",
    "        else:\n",
    "            data[date_name] = getattr(data['date'].dt, date_attr).astype('int16') # note .dt changes date to something thats not datetime64\n",
    "            # returns series indexed like original series and extracts datetime attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>event_type_2_enc</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_350</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>11150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_008_CA_1_evaluation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_009_CA_1_evaluation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id      d  sales       date  wm_yr_wk  ...  event_type_2_enc  lag_7  \\\n",
       "0       CA  d_350      0 2012-01-13     11150  ...          1.387695    NaN   \n",
       "1       CA  d_350      2 2012-01-13     11150  ...          1.387695    NaN   \n",
       "2       CA  d_350      0 2012-01-13     11150  ...          1.387695    NaN   \n",
       "3       CA  d_350      0 2012-01-13     11150  ...          1.387695    NaN   \n",
       "4       CA  d_350      2 2012-01-13     11150  ...          1.387695    NaN   \n",
       "\n",
       "   lag_28  rmean_7_7  rmean_28_7  rmean_7_28  rmean_28_28  week  quarter  days  \n",
       "0     NaN        NaN         NaN         NaN          NaN     2        1    13  \n",
       "1     NaN        NaN         NaN         NaN          NaN     2        1    13  \n",
       "2     NaN        NaN         NaN         NaN          NaN     2        1    13  \n",
       "3     NaN        NaN         NaN         NaN          NaN     2        1    13  \n",
       "4     NaN        NaN         NaN         NaN          NaN     2        1    13  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "create_features(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41571939 entries, 0 to 41571938\n",
      "Data columns (total 40 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                category      \n",
      " 1   item_id           category      \n",
      " 2   dept_id           category      \n",
      " 3   cat_id            category      \n",
      " 4   store_id          category      \n",
      " 5   state_id          category      \n",
      " 6   d                 category      \n",
      " 7   sales             int16         \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           int8          \n",
      " 11  wday              int8          \n",
      " 12  month             int8          \n",
      " 13  year              int16         \n",
      " 14  event_name_1      int8          \n",
      " 15  event_type_1      int8          \n",
      " 16  event_name_2      int8          \n",
      " 17  event_type_2      int8          \n",
      " 18  snap_CA           int8          \n",
      " 19  snap_TX           int8          \n",
      " 20  snap_WI           int8          \n",
      " 21  sell_price        float16       \n",
      " 22  item_id_enc       float16       \n",
      " 23  dept_id_enc       float16       \n",
      " 24  cat_id_enc        float16       \n",
      " 25  store_id_enc      float16       \n",
      " 26  state_id_enc      float16       \n",
      " 27  event_name_1_enc  float16       \n",
      " 28  event_type_1_enc  float16       \n",
      " 29  event_name_2_enc  float16       \n",
      " 30  event_type_2_enc  float16       \n",
      " 31  lag_7             float16       \n",
      " 32  lag_28            float16       \n",
      " 33  rmean_7_7         float16       \n",
      " 34  rmean_28_7        float16       \n",
      " 35  rmean_7_28        float16       \n",
      " 36  rmean_28_28       float16       \n",
      " 37  week              int8          \n",
      " 38  quarter           int8          \n",
      " 39  days              int8          \n",
      "dtypes: category(7), datetime64[ns](1), float16(16), int16(3), int8(13)\n",
      "memory usage: 3.0 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = downcast(df)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39894989, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all NA values \n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/full_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Now that the dataset is ready with its new features I will fit a model to the data and predict the 28-day future sales for all items. The model used here is LightGBM, which is a gradient boosting algorithm using decision trees. Unlike XGBoost where trees are split depth-wise, LightGBM splits the trees leaf-wise and allows better minimization of loss and hence more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define certains things for modelling\n",
    "cat_feats = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id'] + ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "del_cols = ['id', 'date', 'sales', 'd', 'wm_yr_wk', 'weekday'] + cat_feats + ['event_name_1_enc', 'event_type_1_enc', 'event_name_2_enc', 'event_type_2_enc']\n",
    "train_cols = df.columns[~df.columns.isin(del_cols)]\n",
    "\n",
    "# Get evaluation data: X and y\n",
    "X_train = df[train_cols]\n",
    "y_train = df['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement `lightgbm` the dataset needs to be split into training and validation sets. The algorithm fits the model on the train data and validates the model and calculates the RMSE using the valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Create random training and validation datasets\n",
    "np.random.seed(123)\n",
    "\n",
    "# Get 2million random indices (WoR) from X_train and save as validation\n",
    "valid_ind = np.random.choice(X_train.index.values, 2_000_000, replace=False)\n",
    "# The rest of the data used for fitting the model\n",
    "train_ind = np.setdiff1d(X_train.index.values, valid_ind)\n",
    "\n",
    "# Data to fit model, lgb calls this the 'train' data\n",
    "train_data = lgb.Dataset(X_train.loc[train_ind], \n",
    "                         label=y_train.loc[train_ind], \n",
    "                         free_raw_data=False)\n",
    "\n",
    "# Data for validation\n",
    "valid_data = lgb.Dataset(X_train.loc[valid_ind], \n",
    "                         label=y_train.loc[valid_ind], \n",
    "                         free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X and y train datasets\n",
    "# X_train.to_csv('data/X_train.csv',index=False)\n",
    "# y_train.to_csv('data/y_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, X_train, y_train, valid_ind, train_ind\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soumya Chatterjee\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's rmse: 2.88948\n",
      "[40]\tvalid_0's rmse: 2.5748\n",
      "[60]\tvalid_0's rmse: 2.49682\n",
      "[80]\tvalid_0's rmse: 2.47225\n",
      "[100]\tvalid_0's rmse: 2.46098\n",
      "[120]\tvalid_0's rmse: 2.45288\n",
      "[140]\tvalid_0's rmse: 2.44543\n",
      "[160]\tvalid_0's rmse: 2.43845\n",
      "[180]\tvalid_0's rmse: 2.43169\n",
      "[200]\tvalid_0's rmse: 2.4251\n",
      "[220]\tvalid_0's rmse: 2.419\n",
      "[240]\tvalid_0's rmse: 2.41262\n",
      "[260]\tvalid_0's rmse: 2.40685\n",
      "[280]\tvalid_0's rmse: 2.40131\n",
      "[300]\tvalid_0's rmse: 2.3966\n",
      "[320]\tvalid_0's rmse: 2.39277\n",
      "[340]\tvalid_0's rmse: 2.38876\n",
      "[360]\tvalid_0's rmse: 2.38489\n",
      "[380]\tvalid_0's rmse: 2.38153\n",
      "[400]\tvalid_0's rmse: 2.37746\n",
      "[420]\tvalid_0's rmse: 2.37471\n",
      "[440]\tvalid_0's rmse: 2.37166\n",
      "[460]\tvalid_0's rmse: 2.36878\n",
      "[480]\tvalid_0's rmse: 2.36648\n",
      "[500]\tvalid_0's rmse: 2.36367\n",
      "[520]\tvalid_0's rmse: 2.36045\n",
      "[540]\tvalid_0's rmse: 2.35868\n",
      "[560]\tvalid_0's rmse: 2.35617\n",
      "[580]\tvalid_0's rmse: 2.35459\n",
      "[600]\tvalid_0's rmse: 2.3526\n",
      "[620]\tvalid_0's rmse: 2.35058\n",
      "[640]\tvalid_0's rmse: 2.34909\n",
      "[660]\tvalid_0's rmse: 2.34672\n",
      "[680]\tvalid_0's rmse: 2.34458\n",
      "[700]\tvalid_0's rmse: 2.34262\n",
      "[720]\tvalid_0's rmse: 2.34093\n",
      "[740]\tvalid_0's rmse: 2.3395\n",
      "[760]\tvalid_0's rmse: 2.33839\n",
      "[780]\tvalid_0's rmse: 2.33667\n",
      "[800]\tvalid_0's rmse: 2.33576\n",
      "[820]\tvalid_0's rmse: 2.3339\n",
      "[840]\tvalid_0's rmse: 2.33257\n",
      "[860]\tvalid_0's rmse: 2.33159\n",
      "[880]\tvalid_0's rmse: 2.33041\n",
      "[900]\tvalid_0's rmse: 2.32934\n",
      "[920]\tvalid_0's rmse: 2.32851\n",
      "[940]\tvalid_0's rmse: 2.32705\n",
      "[960]\tvalid_0's rmse: 2.32608\n",
      "[980]\tvalid_0's rmse: 2.32481\n",
      "[1000]\tvalid_0's rmse: 2.32351\n",
      "[1020]\tvalid_0's rmse: 2.32256\n",
      "[1040]\tvalid_0's rmse: 2.32177\n",
      "[1060]\tvalid_0's rmse: 2.32075\n",
      "[1080]\tvalid_0's rmse: 2.31961\n",
      "[1100]\tvalid_0's rmse: 2.31855\n",
      "[1120]\tvalid_0's rmse: 2.31787\n",
      "[1140]\tvalid_0's rmse: 2.31703\n",
      "[1160]\tvalid_0's rmse: 2.31614\n",
      "[1180]\tvalid_0's rmse: 2.315\n",
      "[1200]\tvalid_0's rmse: 2.31405\n",
      "[1220]\tvalid_0's rmse: 2.31345\n",
      "[1240]\tvalid_0's rmse: 2.31278\n",
      "[1260]\tvalid_0's rmse: 2.31208\n",
      "[1280]\tvalid_0's rmse: 2.31118\n",
      "[1300]\tvalid_0's rmse: 2.31019\n",
      "[1320]\tvalid_0's rmse: 2.30963\n",
      "[1340]\tvalid_0's rmse: 2.30863\n",
      "[1360]\tvalid_0's rmse: 2.30751\n",
      "[1380]\tvalid_0's rmse: 2.30696\n",
      "[1400]\tvalid_0's rmse: 2.30643\n",
      "[1420]\tvalid_0's rmse: 2.30551\n",
      "[1440]\tvalid_0's rmse: 2.30475\n",
      "[1460]\tvalid_0's rmse: 2.30391\n",
      "[1480]\tvalid_0's rmse: 2.30316\n",
      "[1500]\tvalid_0's rmse: 2.30238\n",
      "Wall time: 21min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Run LightGBM\n",
    "\n",
    "# Hyperparameters\n",
    "params = {'objective':'poisson', \n",
    "          'metric':'rmse', \n",
    "          'force_row_wise':True, \n",
    "          'learning_rate': 0.075, \n",
    "          'bagging_fraction':0.75, \n",
    "          'bagging_freq':1, \n",
    "          'lambda_l2':0.1, \n",
    "          'verbosity':1, \n",
    "          'num_iterations':1500, \n",
    "          'num_leaves':128, \n",
    "          'min_data_in_leaf':100}\n",
    "\n",
    "fit_lgb = lgb.train(params, train_set=train_data, valid_sets=[valid_data], verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1bf03faff48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_lgb.save_model('fit_model_meanenc.lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_lgb = lgb.Booster(model_file='fit_model.lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Using results from the model fit now we can predict the sales for the next 28 days. The test dataset used to predict the model will be created using the `create_data` function written previously. The test dataset starts at day `d_1884` and goes till day `d_1969`. The period `d_1942` to `d_1969` are the 28 days we have to predict. In the prediction process we also use the `create_features` function to create the 7 and 28-day lags and rolling means in the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2016-05-23 00:00:00\n",
      "1 2016-05-24 00:00:00\n",
      "2 2016-05-25 00:00:00\n",
      "3 2016-05-26 00:00:00\n",
      "4 2016-05-27 00:00:00\n",
      "5 2016-05-28 00:00:00\n",
      "6 2016-05-29 00:00:00\n",
      "7 2016-05-30 00:00:00\n",
      "8 2016-05-31 00:00:00\n",
      "9 2016-06-01 00:00:00\n",
      "10 2016-06-02 00:00:00\n",
      "11 2016-06-03 00:00:00\n",
      "12 2016-06-04 00:00:00\n",
      "13 2016-06-05 00:00:00\n",
      "14 2016-06-06 00:00:00\n",
      "15 2016-06-07 00:00:00\n",
      "16 2016-06-08 00:00:00\n",
      "17 2016-06-09 00:00:00\n",
      "18 2016-06-10 00:00:00\n",
      "19 2016-06-11 00:00:00\n",
      "20 2016-06-12 00:00:00\n",
      "21 2016-06-13 00:00:00\n",
      "22 2016-06-14 00:00:00\n",
      "23 2016-06-15 00:00:00\n",
      "24 2016-06-16 00:00:00\n",
      "25 2016-06-17 00:00:00\n",
      "26 2016-06-18 00:00:00\n",
      "27 2016-06-19 00:00:00\n",
      "0 1.023 0.3333333333333333\n",
      "0 2016-05-23 00:00:00\n",
      "1 2016-05-24 00:00:00\n",
      "2 2016-05-25 00:00:00\n",
      "3 2016-05-26 00:00:00\n",
      "4 2016-05-27 00:00:00\n",
      "5 2016-05-28 00:00:00\n",
      "6 2016-05-29 00:00:00\n",
      "7 2016-05-30 00:00:00\n",
      "8 2016-05-31 00:00:00\n",
      "9 2016-06-01 00:00:00\n",
      "10 2016-06-02 00:00:00\n",
      "11 2016-06-03 00:00:00\n",
      "12 2016-06-04 00:00:00\n",
      "13 2016-06-05 00:00:00\n",
      "14 2016-06-06 00:00:00\n",
      "15 2016-06-07 00:00:00\n",
      "16 2016-06-08 00:00:00\n",
      "17 2016-06-09 00:00:00\n",
      "18 2016-06-10 00:00:00\n",
      "19 2016-06-11 00:00:00\n",
      "20 2016-06-12 00:00:00\n",
      "21 2016-06-13 00:00:00\n",
      "22 2016-06-14 00:00:00\n",
      "23 2016-06-15 00:00:00\n",
      "24 2016-06-16 00:00:00\n",
      "25 2016-06-17 00:00:00\n",
      "26 2016-06-18 00:00:00\n",
      "27 2016-06-19 00:00:00\n",
      "1 1.018 0.3333333333333333\n",
      "0 2016-05-23 00:00:00\n",
      "1 2016-05-24 00:00:00\n",
      "2 2016-05-25 00:00:00\n",
      "3 2016-05-26 00:00:00\n",
      "4 2016-05-27 00:00:00\n",
      "5 2016-05-28 00:00:00\n",
      "6 2016-05-29 00:00:00\n",
      "7 2016-05-30 00:00:00\n",
      "8 2016-05-31 00:00:00\n",
      "9 2016-06-01 00:00:00\n",
      "10 2016-06-02 00:00:00\n",
      "11 2016-06-03 00:00:00\n",
      "12 2016-06-04 00:00:00\n",
      "13 2016-06-05 00:00:00\n",
      "14 2016-06-06 00:00:00\n",
      "15 2016-06-07 00:00:00\n",
      "16 2016-06-08 00:00:00\n",
      "17 2016-06-09 00:00:00\n",
      "18 2016-06-10 00:00:00\n",
      "19 2016-06-11 00:00:00\n",
      "20 2016-06-12 00:00:00\n",
      "21 2016-06-13 00:00:00\n",
      "22 2016-06-14 00:00:00\n",
      "23 2016-06-15 00:00:00\n",
      "24 2016-06-16 00:00:00\n",
      "25 2016-06-17 00:00:00\n",
      "26 2016-06-18 00:00:00\n",
      "27 2016-06-19 00:00:00\n",
      "2 1.013 0.3333333333333333\n",
      "(60980, 29)\n",
      "Wall time: 1h 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Prediction\n",
    "alphas = [1.023, 1.018, 1.013] # magic multiplier by kyakovlev\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for iters,(alpha,weight) in enumerate(zip(alphas,weights)):\n",
    "    test = create_data(istrain=False, encode='mean')\n",
    "    cols = [f'F{d}' for d in range(1,29)]\n",
    "    \n",
    "    for tdelta in range(0,28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(tdelta,day)\n",
    "        test_pred = test[(test.date >= day-timedelta(days=max_lags)) & (test.date <= day)].copy()\n",
    "        ## Create features in test data\n",
    "        create_features(test_pred)\n",
    "        ## End creating features in test data\n",
    "        test_pred = test_pred.loc[test_pred.date == day, train_cols]\n",
    "        test.loc[test.date == day, 'sales'] = alpha*fit_lgb.predict(test_pred)\n",
    "        \n",
    "        \n",
    "    test_sub = test.loc[test.date >= fday, ['id', 'sales']].copy()\n",
    "    test_sub['F'] = [f'F{rank}' for rank in test_sub.groupby('id')['id'].cumcount()+1]\n",
    "    test_sub = test_sub.set_index(['id', 'F']).unstack()['sales'][cols].reset_index()\n",
    "#     test_sub.fillna(0., inplace=True)\n",
    "    test_sub.sort_values('id', inplace=True)\n",
    "    test_sub.reset_index(drop=True, inplace=True)\n",
    "    test_sub.to_csv(f'submission_{iters}.csv', index=False)\n",
    "    if iters==0:\n",
    "        sub = test_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += test_sub[cols]*weight\n",
    "    print(iters, alpha, weight)\n",
    "    \n",
    "\n",
    "sub2 = pd.read_csv('data/sales_train_evaluation.csv', usecols=['id']+[f'd_{d}' for d in range(1914, 1914+28)])\n",
    "sub2.rename(columns={f'd_{d}': f'F{d-1913}' for d in range(1914, 1914+28)}, inplace=True)\n",
    "sub2['id'] = sub2['id'].str.replace('evaluation', 'validation')\n",
    "    \n",
    "sub = pd.concat([sub2, sub], axis=0, sort=False)\n",
    "sub.to_csv('lgb_submission.csv', index=False)\n",
    "print(sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2016-05-23 00:00:00\n",
      "1 2016-05-24 00:00:00\n",
      "2 2016-05-25 00:00:00\n",
      "3 2016-05-26 00:00:00\n",
      "4 2016-05-27 00:00:00\n",
      "5 2016-05-28 00:00:00\n",
      "6 2016-05-29 00:00:00\n",
      "7 2016-05-30 00:00:00\n",
      "8 2016-05-31 00:00:00\n",
      "9 2016-06-01 00:00:00\n",
      "10 2016-06-02 00:00:00\n",
      "11 2016-06-03 00:00:00\n",
      "12 2016-06-04 00:00:00\n",
      "13 2016-06-05 00:00:00\n",
      "14 2016-06-06 00:00:00\n",
      "15 2016-06-07 00:00:00\n",
      "16 2016-06-08 00:00:00\n",
      "17 2016-06-09 00:00:00\n",
      "18 2016-06-10 00:00:00\n",
      "19 2016-06-11 00:00:00\n",
      "20 2016-06-12 00:00:00\n",
      "21 2016-06-13 00:00:00\n",
      "22 2016-06-14 00:00:00\n",
      "23 2016-06-15 00:00:00\n",
      "24 2016-06-16 00:00:00\n",
      "25 2016-06-17 00:00:00\n",
      "26 2016-06-18 00:00:00\n",
      "27 2016-06-19 00:00:00\n",
      "(60980, 29)\n",
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "## Prediction\n",
    "# alpha = (1.023+1.018+1.013)/3\n",
    "alpha=1.02 # magic multiplier by kyakovlev\n",
    "sub = 0.\n",
    "\n",
    "test = create_data(istrain=False, encode='mean')\n",
    "cols = [f'F{d}' for d in range(1,29)]\n",
    "\n",
    "for tdelta in range(0,28):\n",
    "    day = fday + timedelta(days=tdelta)\n",
    "    print(tdelta,day)\n",
    "    test_pred = test[(test.date >= day-timedelta(days=max_lags)) & (test.date <= day)].copy()\n",
    "    ## Create features in test data\n",
    "    create_features(test_pred)\n",
    "    ## End creating features in test data\n",
    "    test_pred = test_pred.loc[test_pred.date == day, train_cols]\n",
    "    test.loc[test.date == day, 'sales'] = alpha*fit_lgb.predict(test_pred)\n",
    "\n",
    "\n",
    "sub = test.loc[test.date >= fday, ['id', 'sales']].copy()\n",
    "sub['F'] = [f'F{rank}' for rank in sub.groupby('id')['id'].cumcount()+1]\n",
    "sub = sub.set_index(['id', 'F']).unstack()['sales'][cols].reset_index()\n",
    "#     test_sub.fillna(0., inplace=True)\n",
    "sub.sort_values('id', inplace=True)\n",
    "sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "sub2 = pd.read_csv('data/sales_train_evaluation.csv', usecols=['id']+[f'd_{d}' for d in range(1914, 1914+28)])\n",
    "sub2.rename(columns={f'd_{d}': f'F{d-1913}' for d in range(1914, 1914+28)}, inplace=True)\n",
    "sub2['id'] = sub2['id'].str.replace('evaluation', 'validation')\n",
    "    \n",
    "sub = pd.concat([sub2, sub], axis=0, sort=False)\n",
    "sub.to_csv('lgb_enc_submission.csv', index=False)\n",
    "print(sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>F</th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "F                             id   F1   F2   F3   F4   F5   F6   F7   F8   F9  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  0.0  0.0  0.0  2.0  0.0  3.0  5.0  0.0  0.0   \n",
       "1  HOBBIES_1_002_CA_1_validation  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  HOBBIES_1_003_CA_1_validation  0.0  0.0  1.0  1.0  0.0  2.0  1.0  0.0  0.0   \n",
       "3  HOBBIES_1_004_CA_1_validation  0.0  0.0  1.0  2.0  4.0  1.0  6.0  4.0  0.0   \n",
       "4  HOBBIES_1_005_CA_1_validation  1.0  0.0  2.0  3.0  1.0  0.0  3.0  2.0  3.0   \n",
       "\n",
       "F  ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
       "0  ...  2.0  4.0  0.0  0.0  0.0  0.0  3.0  3.0  0.0  1.0  \n",
       "1  ...  0.0  1.0  2.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  ...  1.0  0.0  2.0  0.0  0.0  0.0  2.0  3.0  0.0  1.0  \n",
       "3  ...  1.0  1.0  0.0  4.0  0.0  1.0  3.0  0.0  2.0  6.0  \n",
       "4  ...  0.0  0.0  0.0  2.0  1.0  0.0  0.0  2.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60980, 30490)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.id.nunique(), sub['id'].str.contains('evaluation$').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
